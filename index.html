---
layout: default
---

<script type="text/javascript" id="MathJax-script" async                        
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">            
</script> 

This website consists of a collection of interactive visualizations about Markov decision processes.<p><p>

A Markov decision process (MDP) can be used to represent stochastic dynamical systems,
and is represented by a tuple \((X, U, R, P)\), where \(X\) is the state-space,
\(U\) is a set of actions, \(R\) is a "reward" function, and \(P\) is a description of stochastic dynamics.<br>
The objective is a to find a policy \(\pi\ : X \rightarrow U\) that maximizes
the sum of rewards over an infinite time horizon (discounted by \(\gamma\)):
$$
\max_{\pi} \mathbb{E}_{u_t \sim \pi(x_t) ,~~  x_{t+1} \sim P(x_{t+1}|x_t, u_t)}\big[ \sum_{t=0}^{\infty} \gamma^t R(x_t, u_t) \big]
$$

This is a very general formulation.<p>
<p>

Examples on this website deal with cases that appeal to a robotics interpretation.<br> 
The state space \(X\) is a discretization of \(\mathbb{R}^2\), \(U\) is a "move direction" (i.e. a discretization of \([0, 2\pi)\) interval), together with a "stay" action, and \(P\) can be summarized by \(x_{t+1} = x_t + u_t\) (possibly with a possibility of ending up in adjacent, but not desired cell).<br>

This is a common way to introduce MDPs which avoids most abstractions and has an easy interpretation: "a robot is navigating in the (grid-)world, and the desired action will not necessarily take it to the desired destination).<br>

When the reward is negative for all \(x\) except some special goal state:
$$ R(x, u) = \begin{cases} 
0 &\text{ if } x = x_{goal} \text{ and } u = 0 \\
< 0 &\text{ o.w. } \end{cases}
$$
then the MDP describes a stochastic shortest path problem.
<br>

This is the scenario shown in this interactive visualization: <a href="example1.html">Single MDP / Single goal</a>.
<p>
<p>

Behavior of an actor following an optimal policy \(\pi^*\) is:
$$
x_{t+1} = x_t + u_t \text{ where } u_t \sim \pi^*(x_t)
$$

The situation is more interesting if they instead have a set of optimal policies \(\{\pi_1, \dots \pi_N\}\) (each a solution to a particular MDP), are following one of them, and are infrequently switching among them.
This is shown in the second visualization: <a href="example2.html">Multiple MDP / Multiple goals</a>.
<p>

